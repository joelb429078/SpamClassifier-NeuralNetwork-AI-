{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classifier\n",
    "\n",
    "## Overview\n",
    "Spam refers to unwanted email, often in the form of advertisements. In the literature, an email that is **not** spam is called *ham*. Most email providers offer automatic spam filtering, where spam emails will be moved to a separate inbox based on their contents. Of course this requires being able to scan an email and determine whether it is spam or ham, a classification problem. This is the subject of this assignment.\n",
    "\n",
    "### Choice of Algorithm\n",
    "While the classification method is a completely free choice, to get a high accuracy I should opt and test various types of models and assess their predicted accuracies to identify the best. Some approaches/models that I could also consider a k-nearest neighbour algorithm, but this may be less accurate. Logistic regression is another option that I may wish to consider. Since I want to look beyond my own current knowledge/skills I might be interested in building something more advanced, like an artificial neural network. This is possible just using `numpy`, but will require significant self-directed learning.\n",
    "\n",
    "**Note:** \n",
    "I will use helper functions in libraries like `numpy` or `scipy`, but I **will not** import code which builds entire models for you. This includes but is not limited to use of libraries like `scikit-learn`, `tensorflow`, or `pytorch`.\n",
    "\n",
    "## Training Data\n",
    "The training data is described below and has 1000 rows. There is also a 500 row set of test data. These are functionally identical to the training data, they are just in a separate csv file to encourage splitting data prior to then training and testing my model. I will consider how to best make use of all available data without overfitting, and to help produce an unbiased estimate for my classifier's accuracy.\n",
    "\n",
    "The cell below loads the training data into a variable called `training_spam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import HTML,Javascript, display\n",
    "\n",
    "training_spam = np.loadtxt(open(\"data/training_spam.csv\"), delimiter=\",\").astype(int)\n",
    "print(\"Shape of the spam training data set:\", training_spam.shape)\n",
    "print(training_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My models training set consists of 1000 rows and 55 columns. Each row corresponds to one email message. The first column is the _response_ variable and describes whether a message is spam `1` or ham `0`. The remaining 54 columns are _features_ that i will use to build a classifier. These features correspond to 54 different keywords (such as \"money\", \"free\", and \"receive\") and special characters (such as \":\", \"!\", and \"$\"). A feature has the value `1` if the keyword appears in the message and `0` otherwise.\n",
    "\n",
    "As mentioned there is also a 500 row set of *test data*. It contains the same 55 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_spam = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\").astype(int)\n",
    "print(\"Shape of the spam testing data set:\", testing_spam.shape)\n",
    "print(testing_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One\n",
    "I will write all of the code for my classifier below this cell. Which showcases my approach and code fully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Merged testing and training data to provide greater dataset \n",
    "#for training NN to allow for better accuracy\n",
    "merged_data = np.concatenate((training_spam, testing_spam), axis=0)\n",
    "train_size = 0.75\n",
    "\n",
    "train_idx = int(len(merged_data) * train_size)\n",
    "training_data = merged_data[:train_idx]\n",
    "testing_data = merged_data[train_idx:]\n",
    "\n",
    "\n",
    "#Feed Forward Neural Network\n",
    "class SpamClassifier:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, learning_rate, num_epochs, lambd):\n",
    "        self.input_size = input_size #number of input featues - 54 words/symbol \n",
    "        self.hidden_sizes = hidden_sizes #number of neurons per hidden layer\n",
    "        self.output_size = output_size #output size - i.e. 0 or 1 \n",
    "        self.learning_rate = learning_rate #learning rate - stepping rate per iteration\n",
    "        self.num_epochs = num_epochs #number of times the entire dataset is iterated through during training\n",
    "        self.lambd = lambd #parameter for the L2 regularisation - implemented to prevent overfitting\n",
    "        self.weights = [] #weights \n",
    "        self.biases = [] #biases\n",
    "        self._initialise_weights()\n",
    "    #intialising random weights to begin with such that they span the entire set\n",
    "    def _initialise_weights(self):\n",
    "        sizes = [self.input_size] + self.hidden_sizes + [self.output_size]\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.weights.append(np.random.randn(sizes[i], sizes[i+1]))\n",
    "            self.biases.append(np.zeros((1, sizes[i+1])))\n",
    "\n",
    "    #sigmoid function - activation function (maps either 1 or 0 inline with binary csv data)\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    #sigmoid derivative \n",
    "    def _sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    #hyperbolic tan function - hidden layers function (maps between -1 & 1)\n",
    "    def _tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    #hyperbolic tan derivative \n",
    "    def _tanh_derivative(self, x):\n",
    "        return 1 - np.square(x)\n",
    "\n",
    "    #forward propogation function \n",
    "    def _forward(self, X):\n",
    "        activations = [X] #passes input data\n",
    "        for i in range(len(self.weights) - 1): #processing all layers apart from last\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            a = self._tanh(z)\n",
    "            activations.append(a) #stored for back propogation\n",
    "        #proceessing the final latyer with sigmoid for output \n",
    "        z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        a = self._sigmoid(z)\n",
    "        activations.append(a)\n",
    "        return activations\n",
    "\n",
    "    #back propogation function\n",
    "    def _backward(self, X, y, activations):\n",
    "        output = activations[-1] #get the output from the last forward pass\n",
    "        error = output - y #working out the output error\n",
    "        deltas = [error * self._sigmoid_derivative(output)] #initialising delta for the output layer\n",
    "        #iterating backwards through all layers to propagate the error\n",
    "        for i in reversed(range(len(activations) - 2)):\n",
    "            delta = np.dot(deltas[0], self.weights[i+1].T) * self._tanh_derivative(activations[i+1])\n",
    "            deltas.insert(0, delta)\n",
    "        #updating weights and biases for all layers using the deltass computed\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * np.dot(activations[i].T, deltas[i]) \n",
    "            self.biases[i] -= self.learning_rate * np.mean(deltas[i], axis=0, keepdims=True)\n",
    "\n",
    "    #Loss function\n",
    "    def _compute_loss(self, y_pred, y_true):\n",
    "        cross_entropy = np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)) #cross-entropy loss\n",
    "        l2_regularisation = 0\n",
    "        #going through per weight for the L2 regularisation\n",
    "        for weight in self.weights:\n",
    "            l2_regularisation += 0.5 * self.lambd * np.sum(np.square(weight)) / len(y_true)\n",
    "        return cross_entropy + l2_regularisation #updated total loss\n",
    "\n",
    "    #training function for the NN\n",
    "    def train(self, data, batch_size=32):\n",
    "        X = data[:, 1:]\n",
    "        y = data[:, 0].reshape(-1, 1)\n",
    "        #Using mini-batch gradient descent\n",
    "        num_batches = len(data) // batch_size #generating batches \n",
    "        #iterating over each epoch\n",
    "        for epoch in range(self.num_epochs):\n",
    "            epoch_loss = 0\n",
    "            epoch_accuracy = 0\n",
    "            #processing through each batch \n",
    "            for batch in range(num_batches):\n",
    "                start = batch * batch_size #start indx\n",
    "                end = start + batch_size #ending index\n",
    "                #labels for current batch\n",
    "                batch_X = X[start:end]\n",
    "                batch_y = y[start:end]\n",
    "                activations = self._forward(batch_X) #forward propagation for the current batch\n",
    "                self._backward(batch_X, batch_y, activations) #backward propagation to update weights and biases\n",
    "                y_pred = activations[-1] #predictions from the last layers activations\n",
    "                batch_loss = self._compute_loss(y_pred, batch_y) # loss for current batch\n",
    "                batch_accuracy = np.mean((y_pred.round() == batch_y).astype(int)) #accuracy for current batch\n",
    "                epoch_loss += batch_loss\n",
    "                epoch_accuracy += batch_accuracy\n",
    "            #avg loss & accuracy\n",
    "            epoch_loss /= num_batches\n",
    "            epoch_accuracy /= num_batches\n",
    "            #outputting the stats per epoch for testing to see \n",
    "            print(f\"Epoch {epoch+1}/{self.num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "    #prediction function\n",
    "    def predict(self, X):\n",
    "        activations = self._forward(X)\n",
    "        return activations[-1].round().flatten()\n",
    "    \n",
    "\n",
    "def create_classifier():\n",
    "    #intialising classifier variable\n",
    "    classifier = SpamClassifier(input_size=54, hidden_sizes = [32, 16], output_size=1, learning_rate=0.015, num_epochs=1200, lambd=0.003)\n",
    "    #to allow it to train --> commented out! As we have the biases and weights preapplied\n",
    "    #classifier.train()\n",
    "    return classifier\n",
    "\n",
    "#initialising and adding the weights & biases\n",
    "classifier = create_classifier()\n",
    "\n",
    "#values for hidden layers \n",
    "hidden_sizes = [32, 16]\n",
    "#initialising best_weight & best_biases arrays\n",
    "best_weights = []\n",
    "best_biases = []\n",
    "\n",
    "#setting up and passing best weights and biases\n",
    "for i in range(len(hidden_sizes) + 1):\n",
    "    weights = np.loadtxt(f\"/bestBiasesWeights/best_weights_layer_{i+1}.csv\", delimiter=\",\")\n",
    "    biases = np.loadtxt(f\"/bestBiasesWeights/best_biases_layer_{i+1}.csv\", delimiter=\",\")\n",
    "    best_weights.append(weights)\n",
    "    best_biases.append(biases)\n",
    "\n",
    "classifier.weights = best_weights\n",
    "classifier.biases = best_biases\n",
    "\n",
    "\n",
    "\n",
    "#function for finding the most optimal weights & biases the data & also finding best weigths and biases\n",
    "#steps for use --> 1)uncomment  2)run the bestvariablesfortraining function 3)comment out the forloop for getting & applying best found weights & biases as well as 2 lines below setting them!\n",
    "\n",
    "\"\"\"\"\n",
    "def evalaccuracy(classifier, testing_data):\n",
    "    test_data = testing_data[:, 1:]\n",
    "    test_labels = testing_data[:, 0]\n",
    "    predictions = classifier.predict(test_data)\n",
    "    accuracy = np.count_nonzero(predictions == test_labels) / test_labels.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "def bestvariablesfortraining(num_runs=100):\n",
    "    best_accuracy = 0\n",
    "    best_weights = None\n",
    "    best_biases = None\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Run {run+1}/{num_runs}\")\n",
    "        classifier = FeedForwardNN(input_size=54, hidden_sizes=[32, 16], output_size=1, learning_rate=0.015, num_epochs=1200, lambd=0.003)\n",
    "        classifier.train(training_data)\n",
    "        accuracy = evalaccuracy(classifier, testing_data)\n",
    "        print(f\"Accuracy on test data: {accuracy}\\n\")\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_weights = classifier.weights\n",
    "            best_biases = classifier.biases\n",
    "\n",
    "    print(f\"Best accuracy: {best_accuracy}\")\n",
    "    print(\"Best weights:\")\n",
    "    for i, weight in enumerate(best_weights):\n",
    "        print(f\"Layer {i+1}: {weight}\")\n",
    "    print(\"Best biases:\")\n",
    "    for i, bias in enumerate(best_biases):\n",
    "        print(f\"Layer {i+1}: {bias}\")\n",
    "\n",
    "    # Write best weights and biases to CSV files\n",
    "    for i, weight in enumerate(best_weights):\n",
    "        np.savetxt(f\"best_weights_layer_{i+1}.csv\", weight, delimiter=\",\")\n",
    "\n",
    "    for i, bias in enumerate(best_biases):\n",
    "        np.savetxt(f\"best_biases_layer_{i+1}.csv\", bias, delimiter=\",\")\n",
    "\n",
    "    return best_weights, best_biases\n",
    "\n",
    "bestvariablesfortraining()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Estimate\n",
    "In the cell below I will put in my estimated accuracy of the model for a more general usecase which exceeds that of the current dataset and may be found in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_accuracy_estimate():\n",
    "    return 0.938  #estimated using multiple runs and the expected value from those runs with an extrapolation on the total sample size of the test dataset (assuming 2000 - 2500 actual test case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Details\n",
    "My classifier will be tested against some hidden data from the same source as the original. The accuracy (percentage of classifications correct) will be calculated, then benchmarked against common methods. \n",
    "\n",
    "#### Test Cell\n",
    "The following code will run your classifier against the provided test data. To enable it, set the constant `SKIP_TESTS` to `False`.\n",
    "\n",
    "**IMPORTANT**: If someone else is viewing this they must set `SKIP_TESTS` back to `True` before submitting this file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_TESTS = True\n",
    "\n",
    "if not SKIP_TESTS:\n",
    "    testing_spam = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\").astype(int)\n",
    "    test_data = testing_spam[:, 1:]\n",
    "    test_labels = testing_spam[:, 0]\n",
    "\n",
    "    predictions = classifier.predict(test_data)\n",
    "    accuracy = np.count_nonzero(predictions == test_labels)/test_labels.shape[0]\n",
    "    print(f\"Accuracy on test data is: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57d38fa58f242d4d856fbaa18e9f8768",
     "grade": false,
     "grade_id": "cell-b6c47ab23c28b2b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "\n",
    "fail = False;\n",
    "\n",
    "success = '\\033[1;32m[✓]\\033[0m'\n",
    "issue = '\\033[1;33m[!]'\n",
    "error = '\\033[1;31m\\t✗'\n",
    "\n",
    "#######\n",
    "##\n",
    "## Skip Tests check.\n",
    "##\n",
    "## Test to ensure the SKIP_TESTS variable is set to True to prevent it slowing down the automarker.\n",
    "##\n",
    "#######\n",
    "\n",
    "if not SKIP_TESTS:\n",
    "    fail = True;\n",
    "    print(\"{} \\'SKIP_TESTS\\' is incorrectly set to False.\\033[0m\".format(issue))\n",
    "    print(\"{} You must set the SKIP_TESTS constant to True in the cell above.\\033[0m\".format(error))\n",
    "else:\n",
    "    print('{} \\'SKIP_TESTS\\' is set to true.\\033[0m'.format(success))\n",
    "\n",
    "#######\n",
    "##\n",
    "## File Name check.\n",
    "##\n",
    "## Test to ensure file has the correct name. This is important for the marking system to correctly process the submission.\n",
    "##\n",
    "#######\n",
    "    \n",
    "p3 = pathlib.Path('./spamclassifier.ipynb')\n",
    "if not p3.is_file():\n",
    "    fail = True\n",
    "    print(\"{} The notebook name is incorrect.\\033[0m\".format(issue))\n",
    "    print(\"{} This notebook file must be named spamclassifier.ipynb\\033[0m\".format(error))\n",
    "else:\n",
    "    print('{} The notebook name is correct.\\033[0m'.format(success))\n",
    "\n",
    "#######\n",
    "##\n",
    "## Create classifier function check.\n",
    "##\n",
    "## Test that checks the create_classifier function exists. The function should train the classifier and return it so that it can be evaluated by the marking system.\n",
    "##\n",
    "#######\n",
    "\n",
    "if \"create_classifier\" not in dir():\n",
    "    fail = True;\n",
    "    print(\"{} The create_classifier function has not been defined.\\033[0m\".format(issue))\n",
    "    print(\"{} Your code must include a create_classifier function as described in the coursework specification.\\033[0m\".format(error))\n",
    "    print(\"{} If you believe you have, \\'restart & run-all\\' to clear this error.\\033[0m\".format(error))\n",
    "else:\n",
    "    print('{} The create_classifier function has been defined.\\033[0m'.format(success))\n",
    "\n",
    "#######\n",
    "##\n",
    "## Classifier variable check.\n",
    "##\n",
    "## Test that checks the classifier variable exists. The marking system will use this variable to make predictions based on a set of random features you have not seen. Your score will be based on how well your classifier predicts the hidden labels.\n",
    "##\n",
    "#######\n",
    "\n",
    "if 'classifier' not in vars():\n",
    "    fail = True;\n",
    "    print(\"{} The classifer variable has not been defined.\\033[0m\".format(issue))\n",
    "    print(\"{} Your code must create a variable called \\'classifier\\' as described in the coursework specification.\\033[0m\".format(error))\n",
    "    print(\"{} This variable should contain the trained classifier you have created.\\033[0m\".format(error))\n",
    "else:\n",
    "    print('{} The classifer variable has been correctly defined.\\033[0m'.format(success))\n",
    "\n",
    "#######\n",
    "##\n",
    "## Accuracy Estimation check.\n",
    "##\n",
    "## Test that checks the accuracy estimation function exists and is a reasonable value. This is a requirement of the coursework specification and is used by the marking system when generating your final grade.\n",
    "##\n",
    "#######\n",
    "\n",
    "if \"my_accuracy_estimate\" not in dir():\n",
    "    fail = True;\n",
    "    print(\"{} The my_accuracy_estimate function has not been defined.\\033[0m\".format(issue))\n",
    "    print(\"{} Your code must include a my_accuracy_estimate function as described in the coursework specification.\\033[0m\".format(error))\n",
    "    print(\"{} If you believe you have, \\'restart & run-all\\' to clear this error.\\033[0m\".format(error))\n",
    "else:\n",
    "    if my_accuracy_estimate() == 0.5:\n",
    "        print(\"{} my_accuracy_estimate function warning.\\033[0m\".format(issue))\n",
    "        print(\"{} my_accuracy_estimate function returns a value of 0.5 - Your classifier is no better than random chance.\\033[0m\".format(error))\n",
    "        print(\"{} Are you sure this is correct.\\033[0m\".format(error))\n",
    "    else:\n",
    "        print('{} The my_accuracy_estimate function has been defined correctly.\\033[0m'.format(success))\n",
    "\n",
    "#######\n",
    "##\n",
    "## Test set check.\n",
    "##\n",
    "## Test that checks your classifier actually works. The calls made here are the same made by the automarker - albeit with different data. If your work fails this test it will score 0 in the automarker.\n",
    "##\n",
    "#######\n",
    "\n",
    "try:\n",
    "    testing_spam = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\").astype(int)\n",
    "    test_data = testing_spam[:, 1:]\n",
    "    test_labels = testing_spam[:, 0]\n",
    "    \n",
    "    try:\n",
    "        predictions = classifier.predict(test_data)\n",
    "        accuracy = np.count_nonzero(predictions == test_labels)/test_labels.shape[0]\n",
    "        print('{0} Success running test set - Accuracy was {1:.2f}%.\\033[0m'.format(success, (accuracy*100)))\n",
    "    except Exception as e:\n",
    "        fail = True\n",
    "        print(\"{} Error running test set.\\033[0m\".format(issue))\n",
    "        print(\"{} Your code produced the following error. This error will result in a zero from the automarker, please fix.\\033[0m\".format(error))\n",
    "#         print(\"{} {}\\033[0m\".format(error, e))\n",
    "        print(e)\n",
    "except:\n",
    "    sys.stderr.write(\"Unable to run one test as the file \\'data/testing_spam.csv\\' could not be found.\")\n",
    "\n",
    "#######\n",
    "##\n",
    "## Final Summary\n",
    "##\n",
    "## Prints the final results of the submission tests.\n",
    "##\n",
    "#######\n",
    "\n",
    "if fail:\n",
    "    sys.stderr.write(\"Your submission is not ready! Please read and follow the instructions above.\")\n",
    "else:\n",
    "    print(\"\\033[1m\\n\\n\")\n",
    "    print(\"╔═══════════════════════════════════════════════════════════════╗\")\n",
    "    print(\"║                        Congratulations!                       ║\")\n",
    "    print(\"║                                                               ║\")\n",
    "    print(\"║            Your work meets all the required criteria          ║\")\n",
    "    print(\"║                   and is ready for submission.                ║\")\n",
    "    print(\"╚═══════════════════════════════════════════════════════════════╝\")\n",
    "    print(\"\\033[0m\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "136737e66133e8cd4881060775030b30",
     "grade": true,
     "grade_id": "cell-b64bc40ab6485b50",
     "locked": true,
     "points": 50,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell. Please do not modify or delete."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
